<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niall Twomey | Neural ODEs with stochastic vector field mixtures</title>
  <meta name="description" content="A website for cataloguing publications, projects, code snippets.">

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">
  <link rel="icon" href="http://localhost:4000/assets/img/favicon.ico">
  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2019/svfm/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
        
        <a href='/'><strong>Niall</strong> Twomey</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="http://localhost:4000/">about</a>

        <!-- Blog -->
        <a class="page-link" href="http://localhost:4000/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="http://localhost:4000/projects/">projects</a>
          
        
          
            <a class="page-link" href="http://localhost:4000/publications/">publications</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="http://localhost:4000/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Neural ODEs with stochastic vector field mixtures</h1>
    <p class="post-meta">May 23, 2019</p>
  </header>
  
  



<div class='img_row'>
  <a href="http://localhost:4000/assets/2019/svfm/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/thumb.png" />
  </a>
</div>



  
    <article class="post-content">
      <p>This blog post relates to some recent work of mine <a class="citation" href="#twomey2019neural">(Twomey, Kozłowski, &amp; Santos-Rodrı́guez Raúl, 2019)</a> on neural ordinary differential equations. While the work is under review at the moment and this post is going to introduce a high-level summary of the main ideas of the paper, and all of the details can be found in the PDF (link can be found at bottom of page).</p>

<h3 id="introduction">Introduction</h3>

<p>The figure below shows a classification problem with two intersecting ‘moons.’ This is a fairly well-known dataset whose topology is relatively simple but the task is still not linearly separable, or in other words you cannot draw a straight line through the figure such that all blue points lie to one side of the line and all orange points lie on the other side.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/moons-start.png">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/moons-start.png" />
  </a>
</div>

<p>But what if we are allowed to subject the data to some kind of transformation before drawing the line? In this post I’ll illustrate how neural ordinary differential equations (NODEs) offer a flexible framework that can make the problem above linearly separable (and more!) without relying on, say, dual representations. We introduce some new ideas to the NODE setting in the paper linked to below and our contributions boil down to two essential questions:</p>

<ol>
  <li>How can we make the transformations simple?</li>
  <li>How can we make the transformations flexible?</li>
</ol>

<p>You may (quite reasonably) think that if you optimise one of these that you will surely have to compromise on the other… but it turns out that with the approach we propose this isn’t the case at all, and you can enjoy the best of both worlds!</p>

<h3 id="example">Example</h3>

<p>The NODE framework is very robust and can subject data to complex and non-linear mappings. The characterisation and complexity of these mappings is optimised during the learning process. Without going into the details (these can be found in the paper) the figure below animates the NODE solution of the moons dataset with some constraints that we introduce in the paper. The main takeaway from this image is that even though the original data is not linearly separable, the final transformation is.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/moons.gif">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/moons.gif" />
  </a>
</div>

<p>The transformations shown above depict the paths travelled by the particles (datapoints) through a vector field over an integration interval. The vector field can be thought of as a force that acts on the particles, and the objective of NODE models is how parametric vector field producing functions can be learnt so that they do something useful.</p>

<h3 id="notation">Notation</h3>

<p>Vector fields are defined as follows in the NODE paradigm</p>

<script type="math/tex; mode=display">~~~~~ \nabla \mathbf{h}(t_i) = f(\mathbf{h}(t_i), t_i; \boldsymbol{\theta})</script>

<p>where <script type="math/tex">\mathbf{h}(t_i)\in\mathbb{R}^D</script> is a hidden state at depth <script type="math/tex">t_i</script> (base state <script type="math/tex">\mathbf{h}(t_0) \triangleq \mathbf{x}</script>, maximal depth <script type="math/tex">t_T</script>), <script type="math/tex">f</script> is a neural network of arbitrary architecture and <script type="math/tex">\boldsymbol{\theta}</script> are its parameters. Subsequent hidden states are derived by taking small steps from <script type="math/tex">\mathbf{h}(t_i)</script> in the direction of the vector field, i.e. <script type="math/tex">\mathbf{h}(t_{i+1}) = \mathbf{h}(t_i) + \delta_{t_i} \nabla \mathbf{h}(t_i)</script>, and the step size <script type="math/tex">\delta_{t_i}</script> is often selected automatically by an ODE solver. The solution of this initial value problem is obtained by repeatedly taking these steps until the output state <script type="math/tex">\mathbf{h}(t_T)</script> is reached, and these outputs may undergo a final transformation, eg through a softmax layer in classification tasks. The optimisation objective is to adjust the dynamics of the ODE through <script type="math/tex">\boldsymbol{\theta}</script> to maximise the data likelihood.</p>

<h3 id="what-makes-transformations-complex">What makes transformations complex?</h3>

<p>Several things contribute to complex paths, and complex paths can not only jepordise model fit but will also require more time (evaluations) to determine solutions. Two complexity-inducing aspects that we penalise in our models are</p>

<ol>
  <li>Paths that span long distances</li>
  <li>Paths that encounter curvature</li>
</ol>

<p>The two animations below illustrate what complex transformations look like on the moons and circles datasets. In particular with the moons dataset, the data can be seen to do an unnecessary rotation before the classes eventually separate. But the feature range of both datasets is scaled significantly due to the transformation. The inner circle has to ‘break through’ the outer circle in the circles example, and this process requires many function evaluations.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/moons-complex.gif">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/moons-complex.gif" />
  </a>
</div>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/nested-circles-complex.gif">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/nested-circles-complex.gif" />
  </a>
</div>

<h3 id="how-to-make-simple-transformations">How to make simple transformations</h3>

<p>It turns out that we can fairly straightforwardly penalise long and curved paths. A loss on the path length can be formulated as follows:</p>

<script type="math/tex; mode=display">~~~~~ \mathcal{L}_{\text{TLoss}} = \frac{1}{T} \sum_{i=1}^T\| \mathbf{h}(t_{i}) - \mathbf{h}(t_{i-1}) \|_2^2</script>

<p>which is simply the sum of the Euclidean distance between successive evaluation points, i.e. the total transportation distance.</p>

<p>It’s reasonably easy to see that high variance vector fields is indicative of a turbulent trajectory while low variance is suggestive of a straight trajectory that is simpler to solve. A loss on the curvature of vector fields will then encourage straight vector fields, and this loss is formulated based on vector field variance as follows:</p>

<script type="math/tex; mode=display">~~~~~ \mathcal{L}_{\text{VLoss}} = \frac{1}{T}\sum_{t=1}^T \left\| \nabla\mathbf{h}(t_i) - \mathbb{E}\left[\nabla \mathbf{h}(t)\right] \right\|^2_2</script>

<p>where <script type="math/tex">\mathbb{E}\left[ \nabla \mathbf{h}(t) \right]</script> is the expectated vector field over the path.</p>

<p>So, by integrating these two penalties into the problem we hope to achieve transformations that have a small total distance traversed and whose transformations are mostly linear. The balance between these path-based losses and the classification objective will ultimately define how simple the actual trajectories are.</p>

<p>Let’s see how successful these methods are in simplifying the solution paths from those shown above. In the animations below, we can see that in the moons dataset a very straightforward transformation was achieved, and only a small fraction of the datapoints actually move. In the circles dataset, the task is much more complex but the task is solved without requiring the inner circle to ‘break’ through the outer circle. Notice as well that the scale of the figure axes does not increase.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/moons-simple.gif">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/moons-simple.gif" />
  </a>
</div>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/nested-circles.gif">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/nested-circles.gif" />
  </a>
</div>

<h3 id="how-to-make-flexible-transformations">How to make flexible transformations</h3>

<p>The key to introducing flexibility to these models is to posit a mixture distribution over a set of vector fields. Instances are assigned to a particular component (vector field) based on, for example, their starting location in space, and instance trajectories are influenced only by that vector field. The figure below illustrates this. The curved line is the boundary for assigning component membership: everything above this line is assigned to one component while everything below this line is assigned to the other. The transparency of the transported datapoints indicates the probability with which they were assigned to a component. The characteristics of component membership are determined automatically by the modelling framework.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/nested-circles-assignment.png">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/nested-circles-assignment.png" />
  </a>
</div>

<p>The datapoints assigned to the respective components travel without being influenced by other vector fields. In a sense, this allows datapoints in different components to ‘pass through’ each other. This is illustrated in the animation below where we can see the orange and blue datapoints pass over one another in opposite directions in the second frame. Of particular note is the speed at which this solution is achieved. All of the animated GIFs in this page have the same frame rate, and since this animation is approximately 2-3 times faster than the others shown the solution was given with 2-3 times fewer evaluation positions.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/nested-circles-svfm.gif">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/nested-circles-svfm.gif" />
  </a>
</div>

<p>Aside from the uncertainty introduced to our model with the mixture distribution, our paper also introduces uncertainty on the vector fields themselves. Our vector fields are probabilistic and (roughly) follow a Gaussian distribution. The effect of this is to produce a one-to-many mapping from the original datapoint to a set of possible endpoints. Our experiments show that this seems to act as a regulariser on the solutions (since in training a wider set of endpoints have been explored) but vector field uncertainty is an essential component for enabling forecasting in human behaviour modelling applications, and these are introduced below.</p>

<h3 id="human-behaviour-modelling">Human behaviour modelling</h3>

<p>In behavioural forecasting we are interested in predicting the future location and trajectory of a person given knowledge of their original starting position. Our models (called stochastic vector field mixtures (SVFMs)) utility in delivering these forecasts is demonstrated with a dataset captured in a residential environment. A volunteer was recorded with a <a href="https://en.m.wikipedia.org/wiki/Lidar">LIDAR</a> data collection unit and the resulting point-cloud was processed with <a href="https://en.m.wikipedia.org/wiki/Simultaneous_localization_and_mapping">SLAM</a> techniques to produce location data on relative reference frame. This dataset effectively captures characteristic behaviour in a home setting, with a particular focus on the types of behaviour following a departure from the living room.</p>

<p>SVFM models are learnt from this dataset. The main difference between this modelling problem and the previous classification problems is that in forecasting every single intermediate evaluation point is and contributes to the loss (and not just the final point, like in classification). In other words, you cannot go from the living room to the kitchen without first traversing through the downstairs hallway.</p>

<p>SVFM models can be samplled since they are probabilistic. A sample in this case depicts a path deemed to be ‘typical’ by the SVFM model. Approximately 20 sample paths are are shown in the image below. Notice that while the broad direction of the paths is shared, every sample follows a slightly different trajectory due to the uncertainty on the vector field.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/behaviour-uniform.png">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/behaviour-uniform.png" />
  </a>
</div>

<p>We also showed that these models can contextualise predictions based on the time of day. The image below shows the forecasted endpoints very late at night and a clear preference for climbing the stairs (i.e. to go to a bedroom) is shown.</p>

<div class="img_row">
  <a href="http://localhost:4000/assets/2019/svfm/behaviour-biased.png">
    <img class="col three" src="http://localhost:4000/assets/2019/svfm/behaviour-biased.png" />
  </a>
</div>

<h3 id="take-away-messages">Take-away messages</h3>

<p>I hope this has given a flavour for some of the capabilities of NODE and NODE-like models (such as SVFM that we propose in the paper below). While I think the classification tasks are very informative for understanding and learning about these model classes (and for motivating useful regularisers), I also believe that the forecasting example delivers a more complete illustration of the model’s capability. This is because neither NODE models (nor a variant called <a href="https://arxiv.org/abs/1904.01681">Augmented NODEs</a>) are capable of solving the behavioural forecasting problem since their advections constitute a homeomorphism. SVFM paths are not so constrained (both by the mixture distribution and the uncertainty on the vector fields) and thus may model a broader set of problems.</p>

<hr />

<ol class="bibliography"><li>

<div id="twomey2019neural">
  
    <span class="title">Neural ODEs with stochastic vector field mixtures</span>
    <span class="author">
      
        
          
            
              <em>Twomey, Niall</em>,
            
          
        
      
        
          
            
              
                <a href="http://www.bristol.ac.uk/engineering/people/michal-kozlowski/index.html" target="_blank">Kozłowski, Michał</a>, 
              
            
          
        
      
        
          
            
              
                and <a href="http://www.raulsantosrodriguez.com/" target="_blank">Santos-Rodrı́guez, Raúl</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:1905.09905</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
    [<a href="http://localhost:4000/assets/pdf/twomey2019neural.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>It was recently shown that neural ordinary differential equation models cannot solve fundamental and seemingly straightforward tasks even with high-capacity vector field representations. This paper introduces two other fundamental tasks to the set that baseline methods cannot solve, and proposes mixtures of stochastic vector fields as a model class that is capable of solving these essential problems. Dynamic vector field selection is of critical importance for our model, and our approach is to propagate component uncertainty over the integration interval with a technique based on forward filtering. We also formalise several loss functions that encourage desirable properties on the trajectory paths, and of particular interest are those that directly encourage fewer expected function evaluations. Experimentally, we demonstrate that our model class is capable of capturing the natural dynamics of human behaviour; a notoriously volatile application area. Baseline approaches cannot adequately model this problem.</p>
  </span>
  
</div>
</li></ol>


    </article>

    
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname  = 'nialltwomey';
        var disqus_identifier = '/blog/2019/svfm';
        var disqus_title      = "Neural ODEs with stochastic vector field mixtures";
        (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
  

</div>

<div class="social">
  
    <div class="col three caption">
      Contact me through the links below
    </div>
  
  <span class="contacticon center">
    <a href="mailto:%74%77%6F%6D%65%79%6E%6A %61%74 %67%6D%61%69%6C %64%6F%74 %63%6F%6D"><i class="fas fa-envelope" style="font-size:0.5em;"></i></a>
    <a href="https://orcid.org/0000-0002-3225-2654" style="font-size:0.5em;" target="_blank" title="ORCID"><i class="ai ai-orcid"></i></a>
    <a href="https://scholar.google.com/citations?user=bRN8Y34AAAAJ" style="font-size:0.5em;" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
    <a href="https://github.com/njtwomey" style="font-size:0.5em;" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
    <a href="https://www.linkedin.com/in/nialltwomey" style="font-size:0.5em;" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
    <a href="https://twitter.com/twomeynj" style="font-size:0.5em;" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
    
    
    
  </span>
</div>



      </div>
    </div>

    <footer>

  <div class="wrapper col three caption">
    &copy; Copyright 2019 Niall Twomey
    
    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
